#!/usr/bin/env python3
"""
Alex Financial Planner – Report Quality Judge.

This module defines the **Judge** agent responsible for evaluating the quality
of financial planning reports generated by the Reporter agent.

Core responsibilities
---------------------
* Take the original instructions, task, and generated report as input
* Use an LLM to assess clarity, completeness, and usefulness of the report
* Return structured feedback and a 0–100 quality score via a Pydantic model
"""

from __future__ import annotations

import logging
import os
from typing import Any

from agents import Agent, Runner
from agents.extensions.models.litellm_model import LitellmModel
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)


# ============================================================
# Evaluation Schema
# ============================================================


class Evaluation(BaseModel):
    """Structured evaluation for a generated financial report.

    Attributes
    ----------
    feedback:
        Natural-language feedback explaining the strengths, weaknesses, and
        overall rationale behind the given score.
    score:
        Numerical score in the range 0–100, where:

        * 0   – extremely poor, misleading, or unusable report
        * 50  – mixed quality, useful in parts but with clear issues
        * 100 – outstanding, highly accurate and well-structured report
    """

    feedback: str = Field(
        description=(
            "Your feedback on the financial report and rationale for your score"
        ),
    )
    score: float = Field(
        description=(
            "Score from 0 to 100 where 0 represents a terrible quality financial "
            "report and 100 represents an outstanding financial report"
        ),
    )


# ============================================================
# Judge Agent – Evaluation Logic
# ============================================================


async def evaluate(
    original_instructions: str,
    original_task: str,
    original_output: str,
) -> Evaluation:
    """Evaluate a financial planning report produced by another agent.

    Parameters
    ----------
    original_instructions:
        The high-level system / role instructions that were given to the
        financial planning agent (e.g. how to structure the report).
    original_task:
        The concrete task or prompt that the planning agent was asked to solve.
    original_output:
        The actual report content produced by the planning agent.

    Returns
    -------
    Evaluation
        A structured :class:`Evaluation` object containing feedback and a score.
    """
    # Model configuration for Bedrock via LiteLLM
    model_id = os.getenv(
        "BEDROCK_MODEL_ID",
        "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
    )
    bedrock_region = os.getenv("BEDROCK_REGION", "us-west-2")

    logger.info("Judge: BEDROCK_REGION from env = %s", bedrock_region)
    os.environ["AWS_REGION_NAME"] = bedrock_region
    logger.info("Judge: Set AWS_REGION_NAME to %s", bedrock_region)

    model = LitellmModel(model=f"bedrock/{model_id}")

    # High-level instructions for the Judge agent
    instructions = (
        "You are an Evaluation Agent that evaluates the quality of a financial "
        "report from a financial planning agent.\n"
        "You will be provided with the instructions that were sent to the "
        "analyst, the task, and the analyst's output. You must evaluate the "
        "quality of the output and explain your reasoning."
    )

    # Per-request task, including all relevant context
    task = f"""
The financial planning agent was given the following instructions:

{original_instructions}

And it was assigned this task:

{original_task}

The financial planning agent's output was:

{original_output}

Evaluate this output and respond with your comments and score.
"""

    try:
        logger.info("Judge: Evaluating financial report")
        agent = Agent(
            name="Judge Agent",
            instructions=instructions,
            model=model,
            output_type=Evaluation,
        )

        result = await Runner.run(agent, input=task, max_turns=5)
        return result.final_output_as(Evaluation)

    except Exception as exc:  # noqa: BLE001
        logger.error("Error evaluating financial report: %s", exc)
        # Fall back to a safe, neutral-ish score with error context in feedback
        return Evaluation(
            feedback=f"Error evaluating financial report: {exc}",
            score=80.0,
        )
